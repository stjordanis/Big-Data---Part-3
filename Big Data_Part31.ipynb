{"cells":[{"cell_type":"markdown","source":["##Introduction\nThe power of Spark which operates on in-memory datasets is the fact that it stores the data as lists using Resilient Distributed Datasets (RDDs) which are themselves distributed in partitions across clusters. RDDs are a fast way of data processing as the data is operated on parallel based on the map-reduce paradigm. RDDs can be be used when the operations are low level. RDDs are typically used on unstructured data like logs or text. For structured and sem-structured data Spark has a higher abstraction called Dataframes.  Handling data through dataframes are extremely fast as they are Optimized using the Catalyst Opimizatin engine and the performance is orders of maganitude faster than RDDs. In addition Dataframes also use Tungsten which handle memory management and garbage collection more effectively"],"metadata":{}},{"cell_type":"markdown","source":["##1. RDD - Select all columns of tables"],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkContext \nrdd = sc.textFile( \"/FileStore/tables/tendulkar.csv\")\nrdd.map(lambda line: (line.split(\",\"))).take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">90</span><span class=\"ansired\">]: </span>\n[[&apos;Runs&apos;,\n  &apos;Mins&apos;,\n  &apos;BF&apos;,\n  &apos;4s&apos;,\n  &apos;6s&apos;,\n  &apos;SR&apos;,\n  &apos;Pos&apos;,\n  &apos;Dismissal&apos;,\n  &apos;Inns&apos;,\n  &apos;Opposition&apos;,\n  &apos;Ground&apos;,\n  &apos;Start Date&apos;],\n [&apos;15&apos;,\n  &apos;28&apos;,\n  &apos;24&apos;,\n  &apos;2&apos;,\n  &apos;0&apos;,\n  &apos;62.5&apos;,\n  &apos;6&apos;,\n  &apos;bowled&apos;,\n  &apos;2&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Karachi&apos;,\n  &apos;15-Nov-89&apos;],\n [&apos;DNB&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;4&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Karachi&apos;,\n  &apos;15-Nov-89&apos;],\n [&apos;59&apos;,\n  &apos;254&apos;,\n  &apos;172&apos;,\n  &apos;4&apos;,\n  &apos;0&apos;,\n  &apos;34.3&apos;,\n  &apos;6&apos;,\n  &apos;lbw&apos;,\n  &apos;1&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Faisalabad&apos;,\n  &apos;23-Nov-89&apos;],\n [&apos;8&apos;,\n  &apos;24&apos;,\n  &apos;16&apos;,\n  &apos;1&apos;,\n  &apos;0&apos;,\n  &apos;50&apos;,\n  &apos;6&apos;,\n  &apos;run out&apos;,\n  &apos;3&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Faisalabad&apos;,\n  &apos;23-Nov-89&apos;]]\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## 1b.RDD - Select columns 1 to 4"],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkContext \nrdd = sc.textFile( \"/FileStore/tables/tendulkar.csv\")\nrdd.map(lambda line: (line.split(\",\")[0:4])).take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">91</span><span class=\"ansired\">]: </span>\n[[&apos;Runs&apos;, &apos;Mins&apos;, &apos;BF&apos;, &apos;4s&apos;],\n [&apos;15&apos;, &apos;28&apos;, &apos;24&apos;, &apos;2&apos;],\n [&apos;DNB&apos;, &apos;-&apos;, &apos;-&apos;, &apos;-&apos;],\n [&apos;59&apos;, &apos;254&apos;, &apos;172&apos;, &apos;4&apos;],\n [&apos;8&apos;, &apos;24&apos;, &apos;16&apos;, &apos;1&apos;]]\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["##1c. RDD - Select specific columns 0, 10"],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkContext \nrdd = sc.textFile( \"/FileStore/tables/tendulkar.csv\")\ndf=rdd.map(lambda line: (line.split(\",\")))\ndf.map(lambda x: (x[10],x[0])).take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">92</span><span class=\"ansired\">]: </span>\n[(&apos;Ground&apos;, &apos;Runs&apos;),\n (&apos;Karachi&apos;, &apos;15&apos;),\n (&apos;Karachi&apos;, &apos;DNB&apos;),\n (&apos;Faisalabad&apos;, &apos;59&apos;),\n (&apos;Faisalabad&apos;, &apos;8&apos;)]\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["##2.  Dataframe - Select all columns"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Read CSV DF').getOrCreate()\ntendulkar1 = spark.read.format('csv').option('header','true').load('/FileStore/tables/tendulkar.csv')\ntendulkar1.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+---+---+---+-----+---+---------+----+----------+----------+----------+\nRuns|Mins| BF| 4s| 6s|   SR|Pos|Dismissal|Inns|Opposition|    Ground|Start Date|\n+----+----+---+---+---+-----+---+---------+----+----------+----------+----------+\n  15|  28| 24|  2|  0| 62.5|  6|   bowled|   2|v Pakistan|   Karachi| 15-Nov-89|\n DNB|   -|  -|  -|  -|    -|  -|        -|   4|v Pakistan|   Karachi| 15-Nov-89|\n  59| 254|172|  4|  0| 34.3|  6|      lbw|   1|v Pakistan|Faisalabad| 23-Nov-89|\n   8|  24| 16|  1|  0|   50|  6|  run out|   3|v Pakistan|Faisalabad| 23-Nov-89|\n  41| 124| 90|  5|  0|45.55|  7|   bowled|   1|v Pakistan|    Lahore|  1-Dec-89|\n+----+----+---+---+---+-----+---+---------+----+----------+----------+----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["##2a. Dataframe- Select specific columns"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Read CSV DF').getOrCreate()\ntendulkar1 = spark.read.format('csv').option('header','true').load('/FileStore/tables/tendulkar.csv')\ntendulkar1.select(\"Runs\",\"BF\",\"Mins\").show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+----+\nRuns| BF|Mins|\n+----+---+----+\n  15| 24|  28|\n DNB|  -|   -|\n  59|172| 254|\n   8| 16|  24|\n  41| 90| 124|\n+----+---+----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["d ##3. Hive QL - Select all columns"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Read CSV DF').getOrCreate()\ntendulkar1 = spark.read.format('csv').option('header','true').load('/FileStore/tables/tendulkar.csv')\ntendulkar1.createOrReplaceTempView('tendulkar1_table')\nspark.sql('select  * from tendulkar1_table limit 5').show(10, truncate = False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+---+---+---+-----+---+---------+----+----------+----------+----------+\nRuns|Mins|BF |4s |6s |SR   |Pos|Dismissal|Inns|Opposition|Ground    |Start Date|\n+----+----+---+---+---+-----+---+---------+----+----------+----------+----------+\n15  |28  |24 |2  |0  |62.5 |6  |bowled   |2   |v Pakistan|Karachi   |15-Nov-89 |\nDNB |-   |-  |-  |-  |-    |-  |-        |4   |v Pakistan|Karachi   |15-Nov-89 |\n59  |254 |172|4  |0  |34.3 |6  |lbw      |1   |v Pakistan|Faisalabad|23-Nov-89 |\n8   |24  |16 |1  |0  |50   |6  |run out  |3   |v Pakistan|Faisalabad|23-Nov-89 |\n41  |124 |90 |5  |0  |45.55|7  |bowled   |1   |v Pakistan|Lahore    |1-Dec-89  |\n+----+----+---+---+---+-----+---+---------+----+----------+----------+----------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["##3a. Hive QL - Select specific columns"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Read CSV DF').getOrCreate()\ntendulkar1 = spark.read.format('csv').option('header','true').load('/FileStore/tables/tendulkar.csv')\ntendulkar1.createOrReplaceTempView('tendulkar1_table')\nspark.sql('select  Runs, BF,Mins from tendulkar1_table limit 5').show(10, truncate = False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+----+\nRuns|BF |Mins|\n+----+---+----+\n15  |24 |28  |\nDNB |-  |-   |\n59  |172|254 |\n8   |16 |24  |\n41  |90 |124 |\n+----+---+----+\n\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["##4. RDD -  Filter rows on specific condition"],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkContext \nrdd = sc.textFile( \"/FileStore/tables/tendulkar.csv\")\ndf=(rdd.map(lambda line: line.split(\",\")[:])\n      .filter(lambda x: x !=\"DNB\")\n      .filter(lambda x: x!= \"TDNB\")\n      .filter(lambda x: x!=\"absent\")\n      .map(lambda x: [x[0].replace(\"*\",\"\")] + x[1:]))\n\ndf.take(5)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">97</span><span class=\"ansired\">]: </span>\n[[&apos;Runs&apos;,\n  &apos;Mins&apos;,\n  &apos;BF&apos;,\n  &apos;4s&apos;,\n  &apos;6s&apos;,\n  &apos;SR&apos;,\n  &apos;Pos&apos;,\n  &apos;Dismissal&apos;,\n  &apos;Inns&apos;,\n  &apos;Opposition&apos;,\n  &apos;Ground&apos;,\n  &apos;Start Date&apos;],\n [&apos;15&apos;,\n  &apos;28&apos;,\n  &apos;24&apos;,\n  &apos;2&apos;,\n  &apos;0&apos;,\n  &apos;62.5&apos;,\n  &apos;6&apos;,\n  &apos;bowled&apos;,\n  &apos;2&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Karachi&apos;,\n  &apos;15-Nov-89&apos;],\n [&apos;DNB&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;4&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Karachi&apos;,\n  &apos;15-Nov-89&apos;],\n [&apos;59&apos;,\n  &apos;254&apos;,\n  &apos;172&apos;,\n  &apos;4&apos;,\n  &apos;0&apos;,\n  &apos;34.3&apos;,\n  &apos;6&apos;,\n  &apos;lbw&apos;,\n  &apos;1&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Faisalabad&apos;,\n  &apos;23-Nov-89&apos;],\n [&apos;8&apos;,\n  &apos;24&apos;,\n  &apos;16&apos;,\n  &apos;1&apos;,\n  &apos;0&apos;,\n  &apos;50&apos;,\n  &apos;6&apos;,\n  &apos;run out&apos;,\n  &apos;3&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Faisalabad&apos;,\n  &apos;23-Nov-89&apos;]]\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark import SparkContext \nrdd = sc.textFile( \"/FileStore/tables/tendulkar.csv\")\ndf=(rdd.map(lambda line: line.split(\",\")[:])\n      .filter(lambda x: x !=\"DNB\")\n      .filter(lambda x: x!= \"TDNB\")\n      .filter(lambda x: x!=\"absent\")\n      .map(lambda x: [x[0].replace(\"*\",\"\")] + x[1:]))\n\ndf.take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">98</span><span class=\"ansired\">]: </span>\n[[&apos;Runs&apos;,\n  &apos;Mins&apos;,\n  &apos;BF&apos;,\n  &apos;4s&apos;,\n  &apos;6s&apos;,\n  &apos;SR&apos;,\n  &apos;Pos&apos;,\n  &apos;Dismissal&apos;,\n  &apos;Inns&apos;,\n  &apos;Opposition&apos;,\n  &apos;Ground&apos;,\n  &apos;Start Date&apos;],\n [&apos;15&apos;,\n  &apos;28&apos;,\n  &apos;24&apos;,\n  &apos;2&apos;,\n  &apos;0&apos;,\n  &apos;62.5&apos;,\n  &apos;6&apos;,\n  &apos;bowled&apos;,\n  &apos;2&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Karachi&apos;,\n  &apos;15-Nov-89&apos;],\n [&apos;DNB&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;-&apos;,\n  &apos;4&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Karachi&apos;,\n  &apos;15-Nov-89&apos;],\n [&apos;59&apos;,\n  &apos;254&apos;,\n  &apos;172&apos;,\n  &apos;4&apos;,\n  &apos;0&apos;,\n  &apos;34.3&apos;,\n  &apos;6&apos;,\n  &apos;lbw&apos;,\n  &apos;1&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Faisalabad&apos;,\n  &apos;23-Nov-89&apos;],\n [&apos;8&apos;,\n  &apos;24&apos;,\n  &apos;16&apos;,\n  &apos;1&apos;,\n  &apos;0&apos;,\n  &apos;50&apos;,\n  &apos;6&apos;,\n  &apos;run out&apos;,\n  &apos;3&apos;,\n  &apos;v Pakistan&apos;,\n  &apos;Faisalabad&apos;,\n  &apos;23-Nov-89&apos;]]\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["##4a. Dataframe - Filter rows on specific condition"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import regexp_replace\nspark = SparkSession.builder.appName('Read CSV DF').getOrCreate()\ntendulkar1 = spark.read.format('csv').option('header','true').load('/FileStore/tables/tendulkar.csv')\ntendulkar1= tendulkar1.where(tendulkar1['Runs'] != 'DNB')\ntendulkar1= tendulkar1.where(tendulkar1['Runs'] != 'TDNB')\ntendulkar1= tendulkar1.where(tendulkar1['Runs'] != 'absent')\ntendulkar1 = tendulkar1.withColumn('Runs', regexp_replace('Runs', '[*]', ''))\ntendulkar1.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+---+---+---+-----+---+---------+----+----------+----------+----------+\nRuns|Mins| BF| 4s| 6s|   SR|Pos|Dismissal|Inns|Opposition|    Ground|Start Date|\n+----+----+---+---+---+-----+---+---------+----+----------+----------+----------+\n  15|  28| 24|  2|  0| 62.5|  6|   bowled|   2|v Pakistan|   Karachi| 15-Nov-89|\n  59| 254|172|  4|  0| 34.3|  6|      lbw|   1|v Pakistan|Faisalabad| 23-Nov-89|\n   8|  24| 16|  1|  0|   50|  6|  run out|   3|v Pakistan|Faisalabad| 23-Nov-89|\n  41| 124| 90|  5|  0|45.55|  7|   bowled|   1|v Pakistan|    Lahore|  1-Dec-89|\n  35|  74| 51|  5|  0|68.62|  6|      lbw|   1|v Pakistan|   Sialkot|  9-Dec-89|\n+----+----+---+---+---+-----+---+---------+----+----------+----------+----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["## 4c Hive QL - Filter rows on specific condition"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Read CSV DF').getOrCreate()\ntendulkar1 = spark.read.format('csv').option('header','true').load('/FileStore/tables/tendulkar.csv')\ntendulkar1.createOrReplaceTempView('tendulkar1_table')\nspark.sql('select  Runs, BF,Mins from tendulkar1_table where Runs NOT IN  (\"DNB\",\"TDNB\",\"absent\")').show(10, truncate = False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+----+\nRuns|BF |Mins|\n+----+---+----+\n15  |24 |28  |\n59  |172|254 |\n8   |16 |24  |\n41  |90 |124 |\n35  |51 |74  |\n57  |134|193 |\n0   |1  |1   |\n24  |44 |50  |\n88  |266|324 |\n5   |13 |15  |\n+----+---+----+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["##5. RDD - Find rows where Runs > 50"],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkContext \nrdd = sc.textFile( \"/FileStore/tables/tendulkar.csv\")\ndf=rdd.map(lambda line: (line.split(\",\")))\ndf=rdd.map(lambda line: line.split(\",\")[0:4]) \\\n   .filter(lambda x: x[0] not in [\"DNB\", \"TDNB\", \"absent\"])\ndf1=df.map(lambda x: [x[0].replace(\"*\",\"\")] + x[1:4])\nheader=df1.first()\ndf2=df1.filter(lambda x: x !=header)\ndf3=df2.map(lambda x: [float(x[0])] +x[1:4])\ndf3.filter(lambda x: x[0]>=50).take(10)\n\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">101</span><span class=\"ansired\">]: </span>\n[[59.0, &apos;254&apos;, &apos;172&apos;, &apos;4&apos;],\n [57.0, &apos;193&apos;, &apos;134&apos;, &apos;6&apos;],\n [88.0, &apos;324&apos;, &apos;266&apos;, &apos;5&apos;],\n [68.0, &apos;216&apos;, &apos;136&apos;, &apos;8&apos;],\n [119.0, &apos;225&apos;, &apos;189&apos;, &apos;17&apos;],\n [148.0, &apos;298&apos;, &apos;213&apos;, &apos;14&apos;],\n [114.0, &apos;228&apos;, &apos;161&apos;, &apos;16&apos;],\n [111.0, &apos;373&apos;, &apos;270&apos;, &apos;19&apos;],\n [73.0, &apos;272&apos;, &apos;208&apos;, &apos;8&apos;],\n [50.0, &apos;158&apos;, &apos;118&apos;, &apos;6&apos;]]\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["##5a. Dataframe - Find rows where Runs >50"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import regexp_replace\nfrom pyspark.sql.types import IntegerType\nspark = SparkSession.builder.appName('Read CSV DF').getOrCreate()\ntendulkar1 = spark.read.format('csv').option('header','true').load('/FileStore/tables/tendulkar.csv')\ntendulkar1= tendulkar1.where(tendulkar1['Runs'] != 'DNB')\ntendulkar1= tendulkar1.where(tendulkar1['Runs'] != 'TDNB')\ntendulkar1= tendulkar1.where(tendulkar1['Runs'] != 'absent')\ntendulkar1 = tendulkar1.withColumn(\"Runs\", tendulkar1[\"Runs\"].cast(IntegerType()))\ntendulkar1.filter(tendulkar1['Runs']>=50).show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+---+---+---+-----+---+---------+----+--------------+------------+----------+\nRuns|Mins| BF| 4s| 6s|   SR|Pos|Dismissal|Inns|    Opposition|      Ground|Start Date|\n+----+----+---+---+---+-----+---+---------+----+--------------+------------+----------+\n  59| 254|172|  4|  0| 34.3|  6|      lbw|   1|    v Pakistan|  Faisalabad| 23-Nov-89|\n  57| 193|134|  6|  0|42.53|  6|   caught|   3|    v Pakistan|     Sialkot|  9-Dec-89|\n  88| 324|266|  5|  0|33.08|  6|   caught|   1| v New Zealand|      Napier|  9-Feb-90|\n  68| 216|136|  8|  0|   50|  6|   caught|   2|     v England|  Manchester|  9-Aug-90|\n 114| 228|161| 16|  0| 70.8|  4|   caught|   2|   v Australia|       Perth|  1-Feb-92|\n 111| 373|270| 19|  0|41.11|  4|   caught|   2|v South Africa|Johannesburg| 26-Nov-92|\n  73| 272|208|  8|  1|35.09|  5|   caught|   2|v South Africa|   Cape Town|  2-Jan-93|\n  50| 158|118|  6|  0|42.37|  4|   caught|   1|     v England|     Kolkata| 29-Jan-93|\n 165| 361|296| 24|  1|55.74|  4|   caught|   1|     v England|     Chennai| 11-Feb-93|\n  78| 285|213| 10|  0|36.61|  4|      lbw|   2|     v England|      Mumbai| 19-Feb-93|\n+----+----+---+---+---+-----+---+---------+----+--------------+------------+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["##6 RDD - groupByKey() and reduceByKey()"],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkContext \nfrom pyspark.mllib.stat import Statistics\nrdd = sc.textFile( \"/FileStore/tables/tendulkar.csv\")\ndf=rdd.map(lambda line: (line.split(\",\")))\ndf=rdd.map(lambda line: line.split(\",\")[0:]) \\\n   .filter(lambda x: x[0] not in [\"DNB\", \"TDNB\", \"absent\"])\ndf1=df.map(lambda x: [x[0].replace(\"*\",\"\")] + x[1:])\nheader=df1.first()\ndf2=df1.filter(lambda x: x !=header)\ndf3=df2.map(lambda x: [float(x[0])] +x[1:])\ndf4 = df3.map(lambda x: (x[10],x[0]))\n#df5=df4.reduceByKey(lambda a,b: a+b,1)\n#df4.groupByKey().mapValues(lambda x: sum(x) / len(x)).take(10)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["##6a Pyspark - Compute mean, min and max"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\ntendulkar1= (sqlContext\n         .read.format(\"com.databricks.spark.csv\")\n         .options(delimiter=',', header='true', inferschema='true')\n         .load(\"/FileStore/tables/tendulkar.csv\"))\ntendulkar1= tendulkar1.where(tendulkar1['Runs'] != 'DNB')\ntendulkar1= tendulkar1.where(tendulkar1['Runs'] != 'TDNB')\ntendulkar1 = tendulkar1.withColumn('Runs', regexp_replace('Runs', '[*]', ''))\ntendulkar1.select('Runs').rdd.distinct().collect()\n\nfrom pyspark.sql import functions as F\ndf=tendulkar1[['Runs','BF','Ground']].groupby(tendulkar1['Ground']).agg(F.mean(tendulkar1['Runs']),F.min(tendulkar1['Runs']),F.max(tendulkar1['Runs']))\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+-----------------+---------+---------+\n       Ground|        avg(Runs)|min(Runs)|max(Runs)|\n+-------------+-----------------+---------+---------+\n    Bangalore|          54.3125|        0|       96|\n     Adelaide|             32.6|        0|       61|\nColombo (PSS)|             37.2|       14|       71|\n Christchurch|             12.0|        0|       24|\n     Auckland|              5.0|        5|        5|\n      Chennai|           60.625|        0|       81|\n    Centurion|             73.5|      111|       36|\n     Brisbane|7.666666666666667|        0|        7|\n   Birmingham|            46.75|        1|       40|\n    Ahmedabad|           40.125|      100|        8|\nColombo (RPS)|            143.0|      143|      143|\n   Chittagong|             57.8|      101|       36|\n    Cape Town|69.85714285714286|       14|        9|\n   Bridgetown|             26.0|        0|       92|\n     Bulawayo|             55.0|       36|       74|\n        Delhi|39.94736842105263|        0|       76|\n   Chandigarh|             11.0|       11|       11|\n Bloemfontein|             85.0|       15|      155|\nColombo (SSC)|77.55555555555556|      104|        8|\n      Cuttack|              2.0|        2|        2|\n+-------------+-----------------+---------+---------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":30}],"metadata":{"name":"Big Data:Part31","notebookId":344430706522733},"nbformat":4,"nbformat_minor":0}
